{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Notebook: RNN for TIME SERIES PREDICTION   #\n",
    "##############################################\n",
    "# Author: Alejandro Benjamin Jimenez Pnata   #\n",
    "# Version: 1.0                               #\n",
    "# Git: alex200420                            #\n",
    "# Python_version: 3.6                        #\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN FOR TIME SERIES PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/11/Sliding-Window-Approach-to-Modeling-Time-Series.png' ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process\n",
    "====================\n",
    "\n",
    "Data preparation\n",
    "---------------------\n",
    "\n",
    "Whether it's a multioutput/input or single output/input timeseries, \n",
    "we need to accommodate some of it to prepare it before training.\n",
    "\n",
    "### Keynotes\n",
    "\n",
    "> It helps to think of it as a sensor-register where each register (at a certain time) gives \"metrics\" or inputs to consider, in that sense, you could be considering multiple time series as inputs, where each time-series is a metric registered by the sensor.\n",
    "> \n",
    "> Remember to be consistent in the time-steps between registers. Meaning that the separation between each register should remain consistent; if it weren't the case, then we'll need to work the data to keep-these records equally-time-distant from each other\n",
    ">\n",
    "> ## Remain consistent\n",
    "\n",
    "**Output**\n",
    "\n",
    "After all the preprocessing, the output should be a dataframe/numpy array with a similar structure as the following(**with 2 metrics**): \n",
    "\n",
    "| Time-step | Metric-1| Metric-2 |\n",
    "|-----------|---------|----------|\n",
    "|   1       |  201.7  |  101.5   |\n",
    "|   2       |  95.7   |  33.2    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<YOUR CODE HERE>##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing input and output\n",
    "---------------------\n",
    "\n",
    "Once the data is preprocessed for each time-step, we will need to\n",
    "prepare the X and Y output, so as to predict the next value(s) for the timeseries.\n",
    "\n",
    "The procedure is as followed:\n",
    "\n",
    "### Procedure\n",
    "> ### Remember to first SORT BY TIME_STEP\n",
    "> For each time-step \"i\" compute an array which contains the data from: $${i-windowsize}: {i}$$ this last ${i}$ step will actually be used to obtain the output for a certain array of inputs . Each input will therefore have: $$singleinputshape =  (windowsize, numfeatures)$$ obtaining, for ${m}$ samples of data: $$inputshape = (m, windowsize, numfeatures)$$\n",
    "> \n",
    "> Were there many diferent outputs to be obtained at once, you should obtain them as you compute the previously mentioned array, all at once\n",
    ">\n",
    "> In order to have better control over Train-Test sets for training, it's recommended to label the dataset beforehand and after the $X$ and $Y$ processing, with the label tag added, split them in $X_{train}, X_{test}, Y_{train}, Y_{test}$\n",
    ">\n",
    "> Values should, finally, be **NORMALIZED** before entering the model, so as to avoid exploding gradient problems because of the loss \n",
    ">\n",
    "> #### Some code for reference is provided below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code\n",
    "\n",
    "\n",
    "```python\n",
    "from tqdm import tqdm # recommended to keep track of process-time per iteration\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\") # warning input type por transformarse a float\n",
    "\n",
    "def X_to_LSTM_ready(X_unpr, window_size = 5):\n",
    "    '''\n",
    "    Takes as input a dataframe.values array an outputs\n",
    "    the computed arrays for a given window_size\n",
    "    param:: window_size default value: 5\n",
    "    param:: X_unpr the array obtained from DataFrameObject.values\n",
    "    '''\n",
    "    l = []\n",
    "    for ind in range(0,int(X_unpr.shape[0])- window_size + 1):\n",
    "        l.append(X_unpr[ind:ind + window_size])\n",
    "    X = np.array(l)\n",
    "    return X\n",
    "\n",
    "df['TrainorTest'] = #define TrainorTest acoording to index values for sklearn.train_test_split on df\n",
    "\n",
    "#=============================== OBTAINING X AND Y ================================#\n",
    "\n",
    "x_features= #as defined\n",
    "reference_features = #as defined including \"TrainorTest\"\n",
    "window_size = #as defined\n",
    "\n",
    "x_ls = []\n",
    "y_ls = []\n",
    "for mat in tqdm(df[\"column_as_unique_identifier\"].unique()):\n",
    "    '''\n",
    "    this FOR SHOULD ONLY be used when a certain column or identifier can be considered to split largely different behaviors in these dataframes. For example, a certain client behaves differently from a different client, or a certain product sells diferently from another one. In a direct TimeSeries is usually not necessary.\n",
    "    '''\n",
    "    \n",
    "    tmp_df = df[df[\"column_as_unique_identifier\"] == mat].sort_values('date_column', ascending = True)\n",
    "    features = tmp_df[x_features + reference_features].copy()\n",
    "    feat_values = features.values\n",
    "    \n",
    "    feats = X_to_LSTM_ready(feat_values, window_size)\n",
    "\n",
    "    tmp_x = feats[:,:,:-len(reference_features)] # Bota Semana y Cantidad Vendida\n",
    "    \n",
    "    tmp_y = feats[:,:,-len(reference_features):][:,-1] #obtain last value from the obtained array\n",
    "    tmp_y = tmp_y.reshape(len(tmp_y), len(reference_features))\n",
    "    \n",
    "    x_ls.append(tmp_x)\n",
    "    y_ls.append(tmp_y)\n",
    "\n",
    "X = np.concatenate(x_ls)\n",
    "y = np.concatenate(y_ls)\n",
    "\n",
    "#=================OBTAINING X_TRAIN, X_TEST, Y_TRAIN, Y_TEST============================#\n",
    "\n",
    "cond_test = y[:, '#index for TrainorTest'] == 'Test'\n",
    "cond_train = y[:,'#index for TrainorTest'] == 'Train'\n",
    "\n",
    "X_train = X[cond_train,:,:]\n",
    "X_test = X[cond_test,:,:]\n",
    "\n",
    "y_train =  y[cond_train, '#indexes needed to output' ]\n",
    "#y_train = y_train.reshape(len(y_train),1) #in case of single output\n",
    "\n",
    "y_test =  y[cond_test, '#indexes needed to output']\n",
    "#y_test = y_test.reshape(len(y_test),1) # in case of single output\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "#=======================NORMALIZING OUTPUT================================#\n",
    "## necessary to train a neural network and avoid exploding gradients because of the loss\n",
    "scalers = {}\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test[:, i, :] = scalers[i].transform(X_test[:, i, :])\n",
    "    \n",
    "    \n",
    "## defining parameters for training\n",
    "batch_size = # as defined (used 100 for 13000 inputs, the value of batch_size should increase if more data is provided so as to avoid under-fitting and training-low-speeds)\n",
    "\n",
    "max_train = len(X_train)//batch_size*batch_size\n",
    "\n",
    "X_train_ready = X_train[:max_train,:,:]\n",
    "y_train_ready = y_train[:max_train,:]\n",
    "\n",
    "max_test = len(X_test)//batch_size*batch_size\n",
    "\n",
    "X_test_ready = X_test[:max_test,:,:]\n",
    "y_test_ready = y_test[:max_test,:]\n",
    "\n",
    "y_test_ready_out = y_test_ready[:,-1].reshape(len(y_test_ready),1)\n",
    "y_train_ready_out = y_train_ready[:,-1].reshape(len(y_train_ready),1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##<YOUR CODE HERE>##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and predict\n",
    "---------------------\n",
    "\n",
    "Now to the interesting part! Training the model.\n",
    "* Read first about LSTM and how they work, example is provided below for guidance\n",
    "\n",
    "* So there's a lot of things to consider in this part, each of which is highly docummented and I would suggest to review topics such as: Finetuning models, Techniques to avoid overfitting, Keras callbacks to obtain best model, Bias and variance balance.\n",
    "\n",
    "### Keynotes\n",
    "> ### Orthogonality\n",
    "> One of the objectives namely on any neural network is to keep the concept of orthogonality in mind. What does this mean? Remember how on a remote control, each button only changes one thing from the configuration on the TV?, so you can change channel, or you can change the volumne, each one independently from each other?. Neural networks don't quite work the same way. So in order to find a best model you should always be careful about what you change and keep a record of the changes you make.\n",
    ">\n",
    ">\n",
    "> ### Bias and Variance\n",
    ">\n",
    "> So bias is basically how my ${training_{accuracy}}$ is far from what i actually want:${bayes_{accuracy}}$, and the variance is how far is my ${test_{accuracy}}$ from my ${training_{accuracy}}$. There is different solutions for each problem(to keep orthogoanlity).\n",
    ">\n",
    "> **In general terms** bias problem can be solved through a bigger dataset, deeper-network or more complex-structured network, whereas the variance problem is solved through regularization, though sometimes the problem may be that the test_set has a different nature from the training_set, so keeping the same nature problem in test and train set is also important\n",
    ">\n",
    "> **Identify if the problem is a classification or regression problem** and choose the loss function and **Last Layer OUTPUT function accordingly**\n",
    ">\n",
    "> Choose a loss function that relates to the objective of the problem.\n",
    ">\n",
    ">**Lastly,**\n",
    "> ### Remember that with sufficient epochs any model can be completely overfitted in the train_set, the goal is to generalize the model and the way to observe that is to see the variance also decreases!\n",
    ">\n",
    "> ### Always solve the bias problem first before the variance problem! and iterate!\n",
    ">\n",
    "> Have Fun!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## This example is done with keras, as it's the simplest way to build a lstm-net\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "import keras.backend as tf\n",
    "from keras import optimizers\n",
    "## model-related-parameters\n",
    "dropout_rate = .15\n",
    "batch_size = #defined in the previous example given that it was necessary for processing the data\n",
    "## defining model structure\n",
    "adam_opt = keras.optimizers.Adam(lr=1e-5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(60, batch_input_shape=(batch_size, 3, 7), stateful=True, return_sequences = True))\n",
    "model.add(LSTM(30, stateful=True, return_sequences = False))\n",
    "model.add(Dense(40, activation = 'relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(20, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid')) # choose appropriate output function depending of whether it's a classification or regression problem\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= adam_opt, metrics = ['acc']) ## choose an appropriate loss function depending on the objective of the problem, remember that the metric 'acc' is NOT MEANINFUL IN REGRESSION PROBLEMS, INSTEAD RMSE, MSE, MAE ARE SUGGESTED\n",
    "##------##\n",
    "model.fit(X_train_ready, y_train_ready_out, validation_data = (X_test_ready, y_test_ready_out), epochs=50, batch_size=batch_size, verbose= True, shuffle=True)\n",
    "\n",
    "y_test_predicted = model.predict(X_test_ready, batch_size = batch_size)\n",
    "\n",
    "##### IF THE EXAMPLE IS A BINARY-CLASSIFICATION PROBLEM YOU MAY CONSIDER THE FOLLOWING ######\n",
    "## CALCULATING ROC-AUC FOR PREDICTED VALUES ####\n",
    "\n",
    "# # roc curve and auc\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from matplotlib import pyplot\n",
    "# # generate 2 class dataset\n",
    "# # calculate scores\n",
    "# lr_auc = roc_auc_score(list(y_test_ready_out.reshape(len(y_test_ready_out),)), list(y_test_predicted.reshape(len(y_test_predicted),)))\n",
    "# # summarize scores\n",
    "# print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# # calculate roc curves\n",
    "# lr_fpr, lr_tpr, _ = roc_curve(list(y_test_ready_out.reshape(len(y_test_ready_out),)), list(y_test_predicted.reshape(len(y_test_predicted),)))\n",
    "# # plot the roc curve for the model\n",
    "# pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# # axis labels\n",
    "# pyplot.xlabel('False Positive Rate')\n",
    "# pyplot.ylabel('True Positive Rate')\n",
    "# # show the legend\n",
    "# pyplot.legend()\n",
    "# # show the plot\n",
    "# pyplot.show()\n",
    "# model.compile(loss= 'mae', optimizer= 'adam', metrics = ['acc'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#< YOUR CODE HERE >#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
